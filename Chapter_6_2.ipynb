{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_6_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO+OWYhRaaBlYO9MQaiMFBT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y-xIh3OXR7GI"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import urllib.request as urllib2\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from PIL import *\n",
        "from PIL import ImageFile\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download Stanford car dataset\n",
        "def getting_data(url, path):\n",
        "    data = urllib2.urlopen(url)\n",
        "    tar_package = tarfile.open(fileobj=data, mode='r:gz')\n",
        "    tar_package.extractall(path)\n",
        "    tar_package.close()\n",
        "    print(\"Data extracted and saved.\")\n",
        "\n",
        "getting_data(\"http://ai.stanford.edu/~jkrause/car196/car_ims.tgz\", \"./\")\n",
        "\n",
        "# download metadata\n",
        "def getting_metadata(url, filename):\n",
        "    labels = urllib2.urlopen(url)\n",
        "    file = open(filename, 'wb')\n",
        "    file.write(labels.read())\n",
        "    file.close()\n",
        "    print(\"Metadata downloaded and saved.\")\n",
        "\n",
        "getting_metadata(\"http://ai.stanford.edu/~jkrause/car196/cars_annos.mat\", \"car_metadata.mat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTT0NJw3STCb",
        "outputId": "0e6bd580-eb91-4d38-a5c2-6e1cc7936689"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data extracted and saved.\n",
            "Metadata downloaded and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MetaParsing():\n",
        "    '''\n",
        "    Class for parsing image and meta-data for the Stanford car dataset to create a custom dataset.\n",
        "    path: The filepah to the metadata in .mat format.\n",
        "    *args: Accepts dictionaries with self-created labels which will be extracted from the metadata (e.g. {0: 'Audi', 1: 'BMW', 3: 'Other').\n",
        "    year: Can be defined to create two classes (<=year and later).\n",
        "    '''\n",
        "    def __init__(self, path, *args, year=None):\n",
        "        # load metadata in matlab format\n",
        "        self.mat = scipy.io.loadmat(path)\n",
        "        # used to create cohort flag\n",
        "        self.year = year\n",
        "        # hold two mapping dictionary(s)\n",
        "        self.args = args\n",
        "        self.annotations = np.transpose(self.mat['annotations'])\n",
        "        # extract the file name for each sample\n",
        "        self.file_names = [annotation[0][0][0].split(\"/\")[-1] for annotation in self.annotations]\n",
        "        # extract the index of the label for each sample\n",
        "        self.label_indices = [annotation[0][5][0][0] for annotation in self.annotations]\n",
        "        # extract the car names as strings\n",
        "        self.car_names = [x[0] for x in self.mat['class_names'][0]]\n",
        "        # create a list with car names instead of label indices for each sample\n",
        "        self.translated_car_names = [self.car_names[x-1] for x in self.label_indices]\n",
        "      \n",
        "    def brand_types(self, base_dict, x):\n",
        "        y = list(base_dict.keys())[-1]\n",
        "        # perform string-based matching\n",
        "        for k,v in base_dict.items():\n",
        "            if v in x: \n",
        "                y=k\n",
        "        return y\n",
        "\n",
        "    def parsing(self):\n",
        "        result = []\n",
        "        # retrieve the indexes of brand and type of vehicle\n",
        "        for arg in self.args:\n",
        "            temp_list = [self.brand_types(arg, x) for x in self.translated_car_names]\n",
        "            result.append(temp_list)\n",
        "        # retrieve the cohort tagging flag\n",
        "        if self.year != None:\n",
        "            years_list = [0 if int(x.split(\" \")[-1]) <= self.year else 1 for x in self.translated_car_names]\n",
        "            result.append(years_list)\n",
        "        return result, self.file_names, self.translated_car_names"
      ],
      "metadata": {
        "id": "gBFhHXg8Saxf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brand_dict = {0: 'Audi', 1: 'BMW', 2: 'Chevrolet', 3: 'Dodge', 4: 'Ford', 5: 'Other'}\n",
        "vehicle_types_dict = {0: 'Convertible', 1: 'Coupe', 2: 'SUV', 3: 'Van', 4: 'Other'}\n",
        "results, file_names, translated_car_names = MetaParsing(\"./car_metadata.mat\", \n",
        "                                brand_dict, vehicle_types_dict, year=2009).parsing()"
      ],
      "metadata": {
        "id": "zfPgy6_7S3Qq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_classes(base_dict, base_list):\n",
        "  for i in range(len(list(base_dict.keys()))):\n",
        "    print(\"{}: {}\".format(base_dict[i], str(base_list.count(i))))\n",
        "# count of brand names\n",
        "count_classes(brand_dict, results[0])\n",
        "# count of type of vehicle\n",
        "count_classes(vehicle_types_dict, results[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyrOlx6zS500",
        "outputId": "a95e36be-0a44-4957-b33b-343a7155ece1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audi: 1169\n",
            "BMW: 1055\n",
            "Chevrolet: 1799\n",
            "Dodge: 1253\n",
            "Ford: 1035\n",
            "Other: 9874\n",
            "Convertible: 1907\n",
            "Coupe: 2143\n",
            "SUV: 2855\n",
            "Van: 832\n",
            "Other: 8448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class CarDataset(Dataset):\n",
        "  def __init__(self, car_path, transform, mapping_dict):\n",
        "    self.path = car_path\n",
        "    self.folder = [x for x in listdir(car_path)]\n",
        "    self.transform = transform\n",
        "    self.mapping_dict = mapping_dict\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.folder)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_loc = os.path.join(self.path, self.folder[idx])\n",
        "    image = Image.open(img_loc).convert('RGB')\n",
        "    single_img = self.transform(image)\n",
        "    # retrieve the corresponding labels\n",
        "    label1 = mapping_dict[self.folder[idx]][0]\n",
        "    label2 = mapping_dict[self.folder[idx]][1]\n",
        "    label3 = mapping_dict[self.folder[idx]][2]\n",
        "    sample = {'image':single_img, 'labels': {'label_brand':label1, 'label_vehicle_type':label2, 'label_cohort':label3}}\n",
        "    return sample   "
      ],
      "metadata": {
        "id": "qI4WwktWS-2Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the overall mapping dictionary\n",
        "mapping_dict = dict(zip(file_names,list(zip(results[0],results[1],results[2]))))\n",
        "# define a chain of pre-processing transformations\n",
        "data_transforms = transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "    ])\n",
        "\n",
        "# create the customized dataset instance\n",
        "cardata = CarDataset(\"./car_ims\", transform=data_transforms, mapping_dict=mapping_dict)\n",
        "\n",
        "# split the data in training and testing\n",
        "train_len = int(cardata.__len__()*0.8)\n",
        "test_len = int(cardata.__len__()*0.2)\n",
        "train_set, val_set = torch.utils.data.random_split(cardata, [train_len, test_len])\n",
        "\n",
        "# create the dataloader for each dataset\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2, drop_last=True)\n",
        "test_loader = DataLoader(val_set, batch_size=16, shuffle=False, num_workers=2, drop_last=True)"
      ],
      "metadata": {
        "id": "Y_TO5HNGTBEZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "N4H22gnqTDUW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample['labels'].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4eN426STHqh",
        "outputId": "e05a2f01-4af6-401c-daf9-ff31d1b0779b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['label_brand', 'label_vehicle_type', 'label_cohort'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Keys in the current batch: {}\".format(sample.keys()))\n",
        "print(\"Size for the images in the current batch: {}\".format(sample['image'].shape))\n",
        "print(\"Size for the brand target in the current batch: {}\".format(sample['labels']['label_brand'].shape))\n",
        "print(\"Brand indexes in the current batch: {}\".format(sample['labels']['label_brand']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBorkMahTJY1",
        "outputId": "1f4b5c70-8daa-4abe-c711-feeda2018c36"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in the current batch: dict_keys(['image', 'labels'])\n",
            "Size for the images in the current batch: torch.Size([16, 3, 224, 224])\n",
            "Size for the brand target in the current batch: torch.Size([16])\n",
            "Brand indexes in the current batch: tensor([5, 2, 3, 2, 5, 5, 3, 2, 4, 0, 5, 5, 5, 1, 1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "# download the pre-trained weights for resnet34 model architecture\n",
        "resnet = models.resnet34(pretrained=True)\n",
        "# access all layers including the last classification head\n",
        "list(resnet.children())[-3:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKji12EKTL0L",
        "outputId": "987c3f01-d783-473f-eb98-ae34175f9d4b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): BasicBlock(\n",
              "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (downsample): Sequential(\n",
              "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     )\n",
              "   )\n",
              "   (1): BasicBlock(\n",
              "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              "   (2): BasicBlock(\n",
              "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "   )\n",
              " ),\n",
              " AdaptiveAvgPool2d(output_size=(1, 1)),\n",
              " Linear(in_features=512, out_features=1000, bias=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the last fully connected layer\n",
        "model_wo_fc = nn.Sequential(*(list(resnet.children())[:-1]))"
      ],
      "metadata": {
        "id": "mRbcvO78TN7n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate output of the current sample batch\n",
        "output_sample = model_wo_fc(sample['image'])\n",
        "print(output_sample.shape)\n",
        "print(torch.flatten(output_sample, 1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "010D7s6jTSby",
        "outputId": "09b3ab47-264a-4d91-ac6b-648a1af50419"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 512, 1, 1])\n",
            "torch.Size([16, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten the output\n",
        "output_sample_flatten = torch.flatten(output_sample, 1)\n",
        "# add another FC layer\n",
        "brand = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=512, out_features=6)\n",
        "        )\n",
        "brand(output_sample_flatten).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62n9pk-iTVTv",
        "outputId": "a9e903ff-7b9b-4f51-e3fe-48472d9c6a78"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultilabelClassifier(nn.Module):\n",
        "    def __init__(self, n_brand, n_vehicle_type, n_cohort):\n",
        "        super().__init__()\n",
        "        # download the backbone architecture\n",
        "        self.resnet = models.resnet34(pretrained=True)\n",
        "        # remove the last FC layer\n",
        "        self.model_wo_fc = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
        "        # define the branching head for brand classification\n",
        "        self.brand = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=512, out_features=n_brand)\n",
        "        )\n",
        "        # define the branching head for vehicle type classification\n",
        "        self.vehicle_type = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=512, out_features=n_vehicle_type)\n",
        "        )\n",
        "        # define the branching head for cohort classification\n",
        "        self.cohort = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features=512, out_features=n_cohort)\n",
        "        )\n",
        "    # define the flow of the model\n",
        "    def forward(self, x):\n",
        "        x = self.model_wo_fc(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return {\n",
        "            'brand': self.brand(x),\n",
        "            'vehicle_type': self.vehicle_type(x),\n",
        "            'cohort': self.cohort(x)\n",
        "        }"
      ],
      "metadata": {
        "id": "zA-0aDQgTYRC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MultilabelClassifier(6, 5, 2).to(device)"
      ],
      "metadata": {
        "id": "vespRBUjTa2N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criterion(loss_func, outputs, pictures):\n",
        "    losses = 0\n",
        "    # sum all individual losses\n",
        "    for i, key in enumerate(outputs):\n",
        "        losses += loss_func(outputs[key], pictures['labels'][f'label_{key}'].to(device))\n",
        "    return losses\n",
        "\n",
        "def training(model, device, lr_rate, num_epochs, train_loader):\n",
        "    losses = []\n",
        "    checkpoint_losses = []\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
        "    n_total_steps = len(train_loader)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    # start model training in a nested loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, pictures in enumerate(train_loader):\n",
        "            # grab input images\n",
        "            images = pictures['image'].to(device)\n",
        "            # obtain model predictions\n",
        "            outputs = model(images)\n",
        "            # calculate current loss\n",
        "            loss = criterion(loss_func, outputs, pictures)\n",
        "            losses.append(loss.item())\n",
        "            # clear historical gradients\n",
        "            optimizer.zero_grad()\n",
        "            # apply autograd\n",
        "            loss.backward()\n",
        "            # perform gradient descent update\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % (n_total_steps) == 0:\n",
        "                checkpoint_loss = torch.tensor(losses).mean().item()\n",
        "                checkpoint_losses.append(checkpoint_loss)\n",
        "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {checkpoint_loss:.4f}')\n",
        "    return checkpoint_losses\n",
        "\n",
        "checkpoint_losses = training(model, device, 0.0001, 10, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFUG4lEZTc5t",
        "outputId": "900d38ae-18c4-4c24-95e7-fd63a3ac10fb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [809/809], Loss: 1.8724\n",
            "Epoch [2/10], Step [809/809], Loss: 1.3812\n",
            "Epoch [3/10], Step [809/809], Loss: 1.0810\n",
            "Epoch [4/10], Step [809/809], Loss: 0.8908\n",
            "Epoch [5/10], Step [809/809], Loss: 0.7647\n",
            "Epoch [6/10], Step [809/809], Loss: 0.6745\n",
            "Epoch [7/10], Step [809/809], Loss: 0.6057\n",
            "Epoch [8/10], Step [809/809], Loss: 0.5502\n",
            "Epoch [9/10], Step [809/809], Loss: 0.5073\n",
            "Epoch [10/10], Step [809/809], Loss: 0.4702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model, dataloader, *args):\n",
        "  all_predictions = torch.tensor([]).to(device)\n",
        "  all_true_labels = torch.tensor([]).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      n_correct = []\n",
        "      n_class_correct = []\n",
        "      n_class_samples = []\n",
        "      n_samples = 0\n",
        "\n",
        "      for arg in args:\n",
        "          n_correct.append(len(arg))\n",
        "          n_class_correct.append([0 for i in range(len(arg))])\n",
        "          n_class_samples.append([0 for i in range(len(arg))])\n",
        "\n",
        "      for pictures in dataloader:\n",
        "          images = pictures['image'].to(device)\n",
        "          outputs = model(images)\n",
        "          # obtain target outputs in a nested list\n",
        "          labels = [pictures['labels'][label].to(device) for label in pictures['labels']]\n",
        "\n",
        "          for i, out in enumerate(outputs):\n",
        "              # retrieve the predicted class with the maximum logit for all samples in the batch\n",
        "              _, predicted = torch.max(outputs[out], 1)\n",
        "              # record the cumulative number of correct predictions\n",
        "              n_correct[i] += (predicted == labels[i]).sum().item()\n",
        "              # get the cumulative number of samples till current batch\n",
        "              if i == 0:\n",
        "                  n_samples += labels[i].size(0)\n",
        "\n",
        "              for k in range(len(predicted)):\n",
        "                  label = labels[i][k]\n",
        "                  pred = predicted[k]\n",
        "                  # get number of correct predictions for each category in each class\n",
        "                  if label == pred:\n",
        "                      n_class_correct[i][label] += 1\n",
        "                  # get total number of predictions for each category in each class\n",
        "                  n_class_samples[i][label] += 1\n",
        "            \n",
        "      return n_correct, n_samples, n_class_correct, n_class_samples\n",
        "\n",
        "def class_acc(n_correct, n_samples, n_class_correct, n_class_samples, class_list):\n",
        "    for i in range(len(class_list)):\n",
        "        print(\"-------------------------------------------------\")\n",
        "        acc = 100.0 * n_correct[i] / n_samples\n",
        "        print(f'Overall class performance: {round(acc,1)} %')\n",
        "        for k in range(len(class_list[i])):\n",
        "            acc = 100.0 * n_class_correct[i][k] / n_class_samples[i][k]\n",
        "            print(f'Accuracy of {class_list[i][k]}: {round(acc,1)} %')\n",
        "    print(\"-------------------------------------------------\")\n",
        "\n",
        "classes_brand = list(brand_dict.values())\n",
        "classes_vehicle_type = list(vehicle_types_dict.values())\n",
        "classes_epoch = ['2009 and earlier','2010 and later']\n",
        "class_list = [classes_brand,classes_vehicle_type,classes_epoch]\n",
        "\n",
        "n_correct, n_samples, n_class_correct, n_class_samples = validation(model, test_loader, \n",
        "                            classes_brand, classes_vehicle_type, classes_epoch)\n",
        "\n",
        "class_acc(n_correct, n_samples, n_class_correct, n_class_samples,class_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut01Z7W6Trzf",
        "outputId": "21c68871-bf7b-496a-cfd3-5598fba27405"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------\n",
            "Overall class performance: 90.8 %\n",
            "Accuracy of Audi: 83.6 %\n",
            "Accuracy of BMW: 89.6 %\n",
            "Accuracy of Chevrolet: 78.6 %\n",
            "Accuracy of Dodge: 84.3 %\n",
            "Accuracy of Ford: 75.7 %\n",
            "Accuracy of Other: 96.2 %\n",
            "-------------------------------------------------\n",
            "Overall class performance: 87.8 %\n",
            "Accuracy of Convertible: 85.0 %\n",
            "Accuracy of Coupe: 67.0 %\n",
            "Accuracy of SUV: 91.9 %\n",
            "Accuracy of Van: 85.1 %\n",
            "Accuracy of Other: 92.4 %\n",
            "-------------------------------------------------\n",
            "Overall class performance: 90.6 %\n",
            "Accuracy of 2009 and earlier: 88.6 %\n",
            "Accuracy of 2010 and later: 91.4 %\n",
            "-------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}